{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('paddle': conda)",
   "display_name": "Python 3.7.7 64-bit ('paddle': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ef235cb22eb9cfcc0b95c52f0e536f391d79b887e254d535e5c55b37af87ed4b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：将AlexNet网络从pytorch转为onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模拟输入，并从torchvision中导入alexnet模型，并加载预训练的参数\n",
    "dummy_input = torch.randn(10, 3, 244, 244, device='cuda')\n",
    "model = torchvision.models.alexnet(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置模型图中输入输出的名字，仅用于显示，对模型没有其他影响。\n",
    "\n",
    "网络的输入由简单的输入列表（即传递给forward（）方法的值）和参数的简单列表组成，可以指定部分名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ['actual_intput_1'] + [\"learned_%d\" % i for i in range(16)]\n",
    "output_names = ['output1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用export 可以导出一个二进制文件，后缀是'onnx'。它保存有模型的结构和参数，将`verbose`设置为`True`可以将计算图打印出来，可以看到我们设置的名字。设置的`learned`指的是每一层的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph(%actual_intput_1 : Float(10, 3, 244, 244),\n      %learned_0 : Float(64, 3, 11, 11),\n      %learned_1 : Float(64),\n      %learned_2 : Float(192, 64, 5, 5),\n      %learned_3 : Float(192),\n      %learned_4 : Float(384, 192, 3, 3),\n      %learned_5 : Float(384),\n      %learned_6 : Float(256, 384, 3, 3),\n      %learned_7 : Float(256),\n      %learned_8 : Float(256, 256, 3, 3),\n      %learned_9 : Float(256),\n      %learned_10 : Float(4096, 9216),\n      %learned_11 : Float(4096),\n      %learned_12 : Float(4096, 4096),\n      %learned_13 : Float(4096),\n      %learned_14 : Float(1000, 4096),\n      %learned_15 : Float(1000)):\n  %17 : Float(10, 64, 60, 60) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_intput_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\n  %18 : Float(10, 64, 60, 60) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %19 : Float(10, 64, 29, 29) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:487:0\n  %20 : Float(10, 192, 29, 29) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %learned_2, %learned_3), scope: AlexNet/Sequential[features]/Conv2d[3] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\n  %21 : Float(10, 192, 29, 29) = onnx::Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %22 : Float(10, 192, 14, 14) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:487:0\n  %23 : Float(10, 384, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %learned_4, %learned_5), scope: AlexNet/Sequential[features]/Conv2d[6] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\n  %24 : Float(10, 384, 14, 14) = onnx::Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %25 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %learned_6, %learned_7), scope: AlexNet/Sequential[features]/Conv2d[8] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\n  %26 : Float(10, 256, 14, 14) = onnx::Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %27 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %learned_8, %learned_9), scope: AlexNet/Sequential[features]/Conv2d[10] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\n  %28 : Float(10, 256, 14, 14) = onnx::Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:487:0\n  %30 : Float(10, 256, 6, 6) = onnx::AveragePool[kernel_shape=[1, 1], strides=[1, 1]](%29), scope: AlexNet/AdaptiveAvgPool2d[avgpool] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:767:0\n  %31 : Float(10, 9216) = onnx::Flatten[axis=1](%30), scope: AlexNet/Sequential[classifier]/Dropout[0] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:806:0\n  %32 : Float(10, 4096) = onnx::Gemm[alpha=1, beta=1, transB=1](%31, %learned_10, %learned_11), scope: AlexNet/Sequential[classifier]/Linear[1] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:1369:0\n  %33 : Float(10, 4096) = onnx::Relu(%32), scope: AlexNet/Sequential[classifier]/Dropout[3] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:806:0\n  %34 : Float(10, 4096) = onnx::Gemm[alpha=1, beta=1, transB=1](%33, %learned_12, %learned_13), scope: AlexNet/Sequential[classifier]/Linear[4] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:1369:0\n  %35 : Float(10, 4096) = onnx::Relu(%34), scope: AlexNet/Sequential[classifier]/ReLU[5] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:911:0\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, transB=1](%35, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6] # /home/szq/anaconda3/envs/paddle/lib/python3.7/site-packages/torch/nn/functional.py:1369:0\n  return (%output1)\n\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, 'alexnet.onnx', verbose=True, input_names=input_names, output_names=output_names, dynamic_axes={'actual_intput_1': [0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跟踪和脚本\n",
    "pytorch导出onnx有两种形式： trace 和 script\n",
    "\n",
    "- trace 是根据输入运行一次模型，只导出和本次输入相关的运算。如模型针对不同的输入有不同的操作，则不能导出其他输入对应的操作。并且对于for和if这样的逻辑判断，也会被展开，逻辑判断中未执行的也不会被导出。 \n",
    "- script 会导出一个ScriptModule，直接从pytorch代码导出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopModel(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        for i in range(y):\n",
    "            x = x + i\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LoopModel()\n",
    "dummy_input = torch.ones(2, 3, dtype=torch.long)\n",
    "loop_count = torch.tensor(5, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1],\n",
       "         [1, 1, 1]]),\n",
       " tensor(5))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "dummy_input, loop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph(%x : Long(2, 3),\n      %y : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%x, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Long(2, 3) = onnx::Add(%7, %8)\n  return (%9)\n\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True, input_names=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用脚本导出来捕捉动态循环，则需要在script中写循环，并在`Module`中调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def loop(x, y):\n",
    "    for i in range(int(y)):\n",
    "        x = x + i \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopModel2(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return loop(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LoopModel2()\n",
    "dummy_input = torch.ones(2, 3, dtype=torch.long)\n",
    "loop_count = torch.tensor(5, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop # <ipython-input-21-d92b6cad4d7d>:3:5\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop # <ipython-input-21-d92b6cad4d7d>:4:13\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)\n\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True, input_names=['input_data', 'loop_range'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的方式，动态循环被正确的捕捉了。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe2.python.onnx.backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load('loop.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = backend.prepare(model)\n",
    "outputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Outputs(_0=array([[37, 37, 37],\n",
       "       [37, 37, 37]], dtype=int64))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([[37, 37, 37],\n       [37, 37, 37]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort_sess = ort.InferenceSession('loop.onnx')\n",
    "outputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n",
    "                              'loop_range': np.array(9).astype(np.int64)})\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导出onnx的函数为：\n",
    "> torch.onnx.export(model, args, f, export_params=True, verbose=False, training=TrainingMode.EVAL, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有参数：\n",
    "\n",
    "- odel 要导出的参数  \n",
    "- args `参数元组或张量`，模型输入参数列表，就是`forward`中需要的输入。所有非张量参数将硬编码到模型，其余的张量输入将作为参数，并且按照在args中的原有顺序排序。  \n",
    "- f 输出文件的名字  \n",
    "- export_params  默认为ture,是否导出模型参数，如果False将导出一个未训练的模型。参数的顺序和`model.state_dict().values()`相同\n",
    "- verbose 默认为False，如果设置为True，将打印Debug信息，成功导出的话，打印的是计算图结构  \n",
    "- training   \n",
    "     - TrainingMode.EVAL 导出模型为推理模式  \n",
    "     - TrainingMode.PRESERVE 如果model.training=False 则导出为推理，否则是训练   \n",
    "     - TrainingMode.TRAINING 导出模型为训练模式  \n",
    "- input_names 输入参数的名字，和输入参数顺序一一对应  \n",
    "- output_names 输出的名字，与输出顺序一一对应  \n",
    "- export_raw_ir和aten，默认False，不建议修改  \n",
    "- operator_export_type 导出操作类型：  \n",
    "    - OperatorExportTypes.ONNX: 所有操作作为常规ONNX操作导出(使用ONNX命名空间).  \n",
    "    - OperatorExportTypes.ONNX_ATEN: 所有操作作为ATEN操作导出 (使用aten命名空间).  \n",
    "    - OperatorExportTypes.ONNX_ATEN_FALLBACK：如果一个操作不被ONNX支持，或者它的symbolic丢失，作为aten操作导出\n",
    "    - OperatorExportTypes.RAW: 导出 rawir\n",
    "    - OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op.\n",
    "- opset_version 默认是9，导出到Onnx的操作版本，可以设置为11,新版本支持更多操作导出。\n",
    "- do_constant_folding 默认为False，常量折叠优化。\n",
    "- example_outputs 默认为None，模型的示例输出，导出一个ScriptModule或TorchScript函数时，必须提供.\n",
    "- dynamic_axes 动态维度，使用字典指定输入输出的动态维度。`key`为输入输出的名字，`value`为将被设置为动态输入的维度索引。通常索引有两种方式指定：\n",
    "    - 一个整数列表，指定动态输入的维度，然后会自动命名这些维度`dynamic_axes = {'input_1':[0, 2, 3],'input_2':[0],'output':[0, 1]}`\n",
    "    - 使用一个字典， 为每个索引命名`dynamic_axes = {'input_1':{0:'batch',1:'width',2:'height'},'input_2':{0:'batch'},'output':{0:'batch',          1:'detections'}`\n",
    "    - 也可以将上面两种方式结合\n",
    "- keep_initializers_as_inputs 如果为True，则导出图形中的所有初始值设定项（通常对应于参数）也将作为输入添加到图形中。如果为False，则初始化器不会作为输入添加到图中，而只将非参数输入添加为输入。\n",
    "- custom_opsets 在导出时指示自定义操作集域和版本的字典。\n",
    "- enable_onnx_checker 如果为True，则onnx模型检查器将作为导出的一部分运行，以确保导出的模型是有效的onnx模型\n",
    "- external_data_format 如果为True，则模型以ONNX外部数据格式导出，在这种情况下，一些模型参数存储在外部二进制文件中，而不是存储在ONNX模型文件本身中。有关格式详细信息，请参阅链接："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}