# 模型并行实践
DataParallel将相同的模型复制到所有GPU，其中每个GPU消耗输入数据的不同分区。虽然它可以显着加快训练过程，但它不适用于模型太大而无法放入单个GPU的某些用例。这篇文章展示了如何通过使用并行模型来解决这个问题，并分享了如何加速模型并行训练的一些见解。  
模型并行的高级概念是将模型的不同子网络放置在不同的设备上，并相应地实现前向方法以跨设备移动中间输出。由于只有模型的一部分可以在任何单个设备上运行，因此一组设备可以共同服务于更大的模型。

## 基本用法
使用包含两个全连接层的简单模型为例，把两个层放在不同的GPU上，移动输入或中间结果匹配模型。
```python
class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()

        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')
        self.relu = torch.nn.ReLU()
        self.net2 = torch.nn.Linear(10, 5).to("cuda:1")

    def forward(self, x):
        x = self.relu(self.net1(x.to("cuda:0")))
        return self.net2(x.to("cuda:1"))
```
