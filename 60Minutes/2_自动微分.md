# AutoGrad：自动微分
Pytorch中的核心是自动微分。`Aurograd`包提供了张量上的自动微分操作，

## Tensor
如果将张量的`.requires_grad`设置为`True`,将会跟踪该张量上进行的所有操作。当计算技术，调用`.backword()`，将会自动计算所有的梯度，并累积到`.grad`属性中。  
使用`.detach()`会阻止对张量历史计算的跟踪，也禁止对未来计算的跟踪。防止跟踪历史，还可以将代码放在`with torch.no_grad():`的代码块中。这样在进行test时，可以停止requires_grad为True的张量自动计算微分。  
另外一个非常重要的概念就是`Function`,`Function`与`Tensor`构成计算图，每个张量都有`.grad_fn`属性，保存创建该张量的`Function`,用户初始创建的输入张量该属性为None。  
通过调用张量的`.backword()`进行自动微分计算，如果是张量T是个标量，则不需要指定任何参数。而张量T包含多个元素，则需要指定参数。