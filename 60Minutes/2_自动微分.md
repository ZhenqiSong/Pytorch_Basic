# AutoGrad：自动微分
Pytorch中的核心是自动微分。`Aurograd`包提供了张量上的自动微分操作，

## Tensor
如果将张量的`.requires_grad`设置为`True`,将会跟踪该张量上进行的所有操作。当计算结束，调用`.backword()`，将会自动计算所有的梯度，并累积到`.grad`属性中。  
使用`.detach()`会阻止对张量历史计算的跟踪，也禁止对未来计算的跟踪。防止跟踪历史，还可以将代码放在`with torch.no_grad():`的代码块中。这样在进行test时，可以停止requires_grad为True的张量自动计算微分。  
另外一个非常重要的概念就是`Function`,`Function`与`Tensor`构成计算图，每个张量都有`.grad_fn`属性，保存创建该张量的`Function`,用户初始创建的输入张量该属性为None。  
通过调用张量的`.backword()`进行自动微分计算，如果是张量T是个标量，则不需要指定任何参数。而张量T包含多个元素，则需要指定参数。

- 创建一个 `requires_grad`为`true`的张量
```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
```
```
out:
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
```
- 执行一个加法运算
```python
y = x + 2
print(y)
print(y.grad_fn)
```
```
out:
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x000002A0AE712550>
```
可以看到这是张量的`grad_fn`属性已经保存了执行的计算函数
- 执行更多的计算
```python
z = y * y * 3
out = z.mean()
print(z)
print(out)
```
```
out:
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>)
tensor(27., grad_fn=<MeanBackward1>)
```
- 默认创建的张量是不记录操作的，使用`requires_grad_(bool)`可以用inplace方式改变属性
```python
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
b = a * a * 3
print(b.requires_grad)
print(b.grad_fn)
```
```
out:
False
True
<MulBackward0 object at 0x000001B262A04A20>
```

## Gradients
因为上面的out是标量，所以可以直接使用`out.backward()`等价于`out.backward(torch.tensor(1.))`
```python
out.backward()
print(x.grad)
```
```
out:
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```
```python
x = torch.randn(3, requires_grad=True)
y = x * 2
while y.data.norm() < 1000:
    y = y * 2
print(y)

# y不再是标量，因为autograd不能自动计算，需要
v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(v)
print(x.grad)
```
```
out:
tensor([-914.8675,  456.0132,  679.7536], grad_fn=<MulBackward0>)
tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])
```
当张量的`requires_grad`属性为True时，可以通过`with torch.no_grad()`阻止自动梯度计算。
```python
print(x.requires_grad)
print((x**2).requires_grad)

with torch.no_grad():
    print((x**2).requires_grad)
```
```
out:
    True
    True
    False
```
> 如果输出的多个loss权重不同的话，例如有三个loss，一个是x loss，一个是y loss，一个是class loss。那么很明显的不可能所有loss对结果影响程度都一样，他们之间应该有一个比例。那么比例这里指的就是[0.1, 1.0, 0.0001]，这个问题中的loss对应的就是上面说的y，那么这里的输出就很好理解了dy/dx=0.1*dy1/dx+1.0*dy2/dx+0.0001*dy3/dx。
