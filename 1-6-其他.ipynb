{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittorchcondaee2ba53616d54d6cad91a7ece1a7d565",
   "display_name": "Python 3.7.7 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "# 数据加载"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "数据载入类：torch.utils.data.DataLoader\n",
    "参数：\n",
    " - dataset: torch.utils.data.Dataset 类的实例\n",
    " - batch_size: 迷你批次的大小\n",
    " - shuffle： 数据会不会被随机打乱\n",
    " - samlpler：自定义的数据采样器，shuffle为true会生成一个默认的。自定义需要设置shuffle为false，每次迭代返回一个数据的下标索引\n",
    " - batch_sampler： 返回一个迷你批次的数据索引\n",
    " - num_workers: 数据载入器使用的进程数目\n",
    " - collate_fn: 定义如何把一批dataset 的实例转换为包含迷你批次数据的张量\n",
    " - pin_memory： 把数据转移到和GPU内存相关联的CPU内存\n",
    " - drop_last: 是否要把最后一个迷你批次的数据丢掉，因为它通常小于设置的batch_size\n",
    " - timeout: 如果大于0，决定在多进程情况下对数据的等待时间\n",
    " - worker_init_fn: 每个数据载入的子进程开始时运行的函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "数据集类型主要有两种：\n",
    "- 映射类型的数据集 torh.utils.data.Dataset\n",
    "- 可迭代类型的数据集 不需要实现__getitem__和__len__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset, self).__init__()\n",
    "        assert end > start, \"this example code only works with end >= start\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None: # 单进程数据载入\n",
    "            iter_start = self.start\n",
    "            iter_end = self.end\n",
    "        else:\n",
    "            per_workder = int(math.ceil((self.end-self.start) / float(worker_info.num_works)))\n",
    "            worker_id = worker_info.worker_id\n",
    "            iter_start = self.start + worker_id * per_workder\n",
    "            iter_end = min(iter_start + per_workder, self.end)\n",
    "        \n",
    "        return iter(range(iter_start, iter_end))"
   ]
  },
  {
   "source": [
    "## 模型和张量的序列化和反序列化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj, f, pickle_module=pickle, pickle_protocol=2)\n",
    "torch.load(f, map_location=None, pickle_module=pickle, **pickle_load_args)"
   ]
  },
  {
   "source": [
    "默认的序列化库是pickle,pickle_protocol是序列化协议"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 分布式数据并行化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "torch.distributed 包含用于分布式计算API，主要支持：\n",
    "- Gloo 支持CPU，GPU，对GPU支持差\n",
    "- MPI 只支持CPU\n",
    "- NCCL 只支持GPU\n",
    "\n",
    "为了启动不同的进程，首先要对所有计算进程进行初始化。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')"
   ]
  },
  {
   "source": [
    "- backend 分别传入三个字符串 gloo， mpi， nccl\n",
    "- init_method 值为None，默认采用 env:// 来启动进程，即每个进程会从当前的环境变量中获取当前节点和其他节点的信息。 其他方法可以通过`tcp://ip:port`的方式来哦获取其他进程的信息\n",
    "- timeout 操作的超时时间，只对gloo有效\n",
    "- store 设置了，init_method会失效，通过传入字典的键值对来指定节点信息\n",
    "- word_size 进程数目\n",
    "- rank 当前进程的序号\n",
    "- group_name 进程组的名字已废弃"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布式数据加载\n",
    "train_dataset = datasets.ImageFolder(traindir, transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    "))\n",
    "\n",
    "train_sampler =  torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=(train_simpleer is None),\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.parallel.DistributedDataParallel(module=module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True,\n",
    "process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False)"
   ]
  },
  {
   "source": [
    "- module 需要被分布式数据并行化的模型\n",
    "- device_ids 分布式GPU的id\n",
    "- output_device 输出的设备\n",
    "- dim 分割的维度\n",
    "- broadcast_buffers 模型是够会在每次前向计算的时候广播模型的缓冲区数据\n",
    "- process_group 创建的进程组\n",
    "- bucket_cap_mb 决定的是参数分块的大小\n",
    "- find_unused_parameters True 找出模块中是否有一些参数没有在规约时被用到\n",
    "- check_reduction 设置True， 保证在下一步的前向计算开始时，上一步的反向一定会完成\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "使用多进程时，为了防止输出竞争的产生，一般方法是判断当前的进程ID是否为0，如果是，则调用IO函数进行输出，否则不进行输出"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ard.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank%ngpus_per_node==0):\n",
    "    save()"
   ]
  }
 ]
}