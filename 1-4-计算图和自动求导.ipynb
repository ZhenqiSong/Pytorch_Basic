{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3a362550ed156c021e5b84a71908e939813efb7a71eaf1c23eb41d081a06eda9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "在创建张量的时候设置一个参数`requires_grad=True`，意味着这个张量会加入到计算图中，作为计算图的叶子节点参与计算。\n",
    "每个张量都有一个grad_fn方法，这个方法包含着创建该张量的运算的导数信息。在反向传播过程中，通过传入后一层的神经网络的梯度，该函数会计算出参与计算的所有张量的梯度。grad_fn本身有一个next_functions属性，包含连接该张量的其他张量grad_fn。通过不断反向传播回溯中间张量的计算节点，可以得到所有张量的梯度。一个张量的梯度张量的信息保存在该张量的grad属性中。\n",
    "\n",
    "此外还有一个专门求导的包，即`torch.autograd`，它有两个重要的函数`torch.autograd.backward`和`torch.autograd.grad`。torch.autograd.backward函数通过传入根结点张量，以及初始梯度张量，可以计算产生该根结点所有对应的叶子节点的梯度。如果要在反向传播的时候保留计算图，可以设置`retain_graph=True`。如果需要创建方向计算图，可以设置`create_graph=True`。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.randn(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.4973, -0.7848, -0.7087],\n",
       "        [ 1.8447,  0.4992, -0.4637],\n",
       "        [ 0.9885, -0.8224, -0.1643]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t1.pow(2).sum() # 计算张量的所有分量平方和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.backward() # 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 2.9945, -1.5696, -1.4173],\n",
       "        [ 3.6894,  0.9983, -0.9274],\n",
       "        [ 1.9770, -1.6447, -0.3286]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "t1.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "t1.grad.zero_() # 单个张量清零梯度的方法"
   ]
  },
  {
   "source": [
    "有时候，不需要求出当前张量对所有产生该张量的叶子节点的梯度名，这是可以使用torch.autograd.grad函数。这个函数的参数是两个张量，第一个张量是计算图的数据结果张量，第二个张量是需要对计算图求导的张量。最后输出的结果是第一个张量对第二个张量求导的结果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t1.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.0481, -3.4426,  0.9009],\n",
       "         [-0.8894,  0.0872, -1.7452],\n",
       "         [ 2.0138, -1.4716, -1.6325]]),)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "torch.autograd.grad(t2, t1)"
   ]
  },
  {
   "source": [
    "# 计算图构建的启用和禁用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "由于计算图的构建需要消耗内存和计算资源，在一些情况下，计算图并不是必要的，可以在`torch.no_grad`上下文管理器中就不会构建。\n",
    "\n",
    "另外，还有一个情况是对于一个张量，我们在反向传播的时候可能不需要让梯度通过这个张量的节点，也就是新建的计算图要和原来的计算图分离。这种情况下，可以使用张量的detach方法，通过调用这个方法，可以返回一个新的张量，该张量会成为一个新的计算图的叶子节点，新的计算图和老得计算图相互分离，互不影响。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7787, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t3 = t1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7787)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7787)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "t1.sum().detach() # 和原来的计算图分离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}