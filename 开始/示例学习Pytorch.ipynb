{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch的两个核心特征：\n",
    "- 类似numpy的多维张量，可以在GPU上运行\n",
    "- 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "## numpy\n",
    "\n",
    "介绍pytorch之前，首先使用numpy实现一个网络。\n",
    "\n",
    "numpy提供了多维张量和操作，但是对深度学习不友好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N是Batch size, H是隐藏层大小\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建输入和输出数据\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31883824.45931278\n",
      "1 30635015.13647095\n",
      "2 37077019.523745716\n",
      "3 44578896.970277786\n",
      "4 44497988.159341395\n",
      "5 31914450.303799387\n",
      "6 16186571.800548442\n",
      "7 6540387.998912735\n",
      "8 2806179.8782955\n",
      "9 1545144.1507705613\n",
      "10 1073461.460157258\n",
      "11 842369.4445914896\n",
      "12 695416.463790141\n",
      "13 586386.928640781\n",
      "14 499905.2994029885\n",
      "15 429238.22338585224\n",
      "16 370695.64591995144\n",
      "17 321801.60374993173\n",
      "18 280741.5532091633\n",
      "19 245948.10557674846\n",
      "20 216282.3937686798\n",
      "21 190871.14337591833\n",
      "22 168998.76886730813\n",
      "23 150095.780800678\n",
      "24 133694.21748948944\n",
      "25 119409.08971889189\n",
      "26 106927.42906132335\n",
      "27 95998.27727163803\n",
      "28 86393.41371239997\n",
      "29 77914.4885381542\n",
      "30 70408.37043025033\n",
      "31 63746.01286338222\n",
      "32 57817.49554206335\n",
      "33 52536.87415776809\n",
      "34 47815.26626447074\n",
      "35 43585.37194325829\n",
      "36 39788.70370580571\n",
      "37 36372.524751473735\n",
      "38 33293.0512679896\n",
      "39 30517.122633058385\n",
      "40 28009.33331298832\n",
      "41 25737.751315303864\n",
      "42 23675.81401928372\n",
      "43 21801.027403981432\n",
      "44 20090.676075399144\n",
      "45 18532.22053331019\n",
      "46 17110.417815904526\n",
      "47 15811.413827262537\n",
      "48 14623.298803142126\n",
      "49 13535.520308798204\n",
      "50 12538.041668488468\n",
      "51 11622.823781670562\n",
      "52 10782.023153808386\n",
      "53 10008.826849601393\n",
      "54 9297.117302803832\n",
      "55 8641.57111612337\n",
      "56 8037.091754530123\n",
      "57 7479.255663310954\n",
      "58 6963.760419030636\n",
      "59 6487.279895723495\n",
      "60 6046.70944324736\n",
      "61 5638.898120583573\n",
      "62 5261.457881231763\n",
      "63 4911.694303234558\n",
      "64 4587.302034935185\n",
      "65 4286.217567336652\n",
      "66 4006.5909773500093\n",
      "67 3746.7666106527386\n",
      "68 3505.2193961099993\n",
      "69 3280.4931883567447\n",
      "70 3071.351397906399\n",
      "71 2876.533259896767\n",
      "72 2695.1319881836744\n",
      "73 2525.9801620384205\n",
      "74 2368.238540779537\n",
      "75 2221.0926473333784\n",
      "76 2083.743787121464\n",
      "77 1955.4899471648555\n",
      "78 1835.6859198043555\n",
      "79 1723.7105735783184\n",
      "80 1619.05778492433\n",
      "81 1521.1789629288664\n",
      "82 1429.594144615803\n",
      "83 1343.8835016719404\n",
      "84 1263.7125874268995\n",
      "85 1188.6509935516908\n",
      "86 1118.2947423733108\n",
      "87 1052.3624370279003\n",
      "88 990.5654530854088\n",
      "89 932.6128217387825\n",
      "90 878.2554921630938\n",
      "91 827.2537450086954\n",
      "92 779.3771032874365\n",
      "93 734.4518894605471\n",
      "94 692.2456557708895\n",
      "95 652.6005853727779\n",
      "96 615.3539366772613\n",
      "97 580.3492069839048\n",
      "98 547.45693005565\n",
      "99 516.5178203285085\n",
      "100 487.4197536248613\n",
      "101 460.0479571887787\n",
      "102 434.2964081222909\n",
      "103 410.08532497123883\n",
      "104 387.2911301651758\n",
      "105 365.8286408964359\n",
      "106 345.6158022915415\n",
      "107 326.57554166715965\n",
      "108 308.6307514942166\n",
      "109 291.7172182401642\n",
      "110 275.7747142999034\n",
      "111 260.7420049576058\n",
      "112 246.56795475414054\n",
      "113 233.1980570852967\n",
      "114 220.58637519088046\n",
      "115 208.68753118503395\n",
      "116 197.45712137694426\n",
      "117 186.85755072527363\n",
      "118 176.8502599033843\n",
      "119 167.40360304295504\n",
      "120 158.48432303312504\n",
      "121 150.05742516404334\n",
      "122 142.09706787013383\n",
      "123 134.577257430839\n",
      "124 127.47141985321139\n",
      "125 120.75530871053279\n",
      "126 114.40726749246758\n",
      "127 108.40561305505432\n",
      "128 102.73251139820412\n",
      "129 97.36692981827832\n",
      "130 92.29268664582301\n",
      "131 87.49295956138394\n",
      "132 82.95133721539328\n",
      "133 78.65445577692931\n",
      "134 74.58854370569861\n",
      "135 70.74517612011041\n",
      "136 67.10553034504926\n",
      "137 63.66002415578109\n",
      "138 60.397855734178535\n",
      "139 57.308305782808475\n",
      "140 54.38308751684791\n",
      "141 51.61199654348363\n",
      "142 48.98758288459204\n",
      "143 46.500708605218804\n",
      "144 44.14435770094833\n",
      "145 41.91068398436817\n",
      "146 39.794260666083844\n",
      "147 37.788220341539855\n",
      "148 35.886281149885455\n",
      "149 34.08421969997728\n",
      "150 32.374492421012135\n",
      "151 30.753346386835606\n",
      "152 29.215704418863517\n",
      "153 27.757440789968136\n",
      "154 26.374032476766686\n",
      "155 25.06171262064169\n",
      "156 23.817036458200597\n",
      "157 22.63573738828003\n",
      "158 21.51478649375888\n",
      "159 20.450669884842412\n",
      "160 19.440650966052274\n",
      "161 18.482072562559473\n",
      "162 17.572020119407526\n",
      "163 16.708226913397443\n",
      "164 15.888171842376902\n",
      "165 15.1097661752633\n",
      "166 14.370409962676252\n",
      "167 13.668109383870446\n",
      "168 13.00105501482802\n",
      "169 12.367474632779183\n",
      "170 11.765588217110459\n",
      "171 11.19362466143491\n",
      "172 10.650315901451556\n",
      "173 10.133899445496555\n",
      "174 9.64314979846582\n",
      "175 9.176672070831874\n",
      "176 8.733555458001803\n",
      "177 8.31212429313133\n",
      "178 7.911495421686057\n",
      "179 7.530675310684358\n",
      "180 7.168632126106336\n",
      "181 6.824329238246274\n",
      "182 6.497029470831269\n",
      "183 6.1857311966577235\n",
      "184 5.889627494624001\n",
      "185 5.607998477471622\n",
      "186 5.340180431099471\n",
      "187 5.085395005331181\n",
      "188 4.843041011896998\n",
      "189 4.612505824152509\n",
      "190 4.3931122150173\n",
      "191 4.184379235824383\n",
      "192 3.98573837701628\n",
      "193 3.7967431381535692\n",
      "194 3.6168804904694873\n",
      "195 3.445738985074078\n",
      "196 3.282804834006151\n",
      "197 3.1277244111980216\n",
      "198 2.9800936968305107\n",
      "199 2.839577917201119\n",
      "200 2.7058051099009948\n",
      "201 2.5784685626626054\n",
      "202 2.4572179094835502\n",
      "203 2.341749507694645\n",
      "204 2.231798124526681\n",
      "205 2.1271206384402106\n",
      "206 2.0274222817625303\n",
      "207 1.9324914833496267\n",
      "208 1.8420845373378223\n",
      "209 1.7559529918843375\n",
      "210 1.6739076537234685\n",
      "211 1.595784010341454\n",
      "212 1.5213467435941128\n",
      "213 1.4504387943976036\n",
      "214 1.3829125185372102\n",
      "215 1.31854831054112\n",
      "216 1.257225487526338\n",
      "217 1.1988084953501024\n",
      "218 1.1431416003503934\n",
      "219 1.0900966373640877\n",
      "220 1.0395674471136407\n",
      "221 0.9913934423323668\n",
      "222 0.9454827223322219\n",
      "223 0.9017375281359492\n",
      "224 0.8600329171984954\n",
      "225 0.8202830401383825\n",
      "226 0.7824107166092136\n",
      "227 0.7463022722684431\n",
      "228 0.7118816303622119\n",
      "229 0.6790725634801489\n",
      "230 0.6477866211891372\n",
      "231 0.6179603479717579\n",
      "232 0.5895346703973563\n",
      "233 0.5624293399751703\n",
      "234 0.5365833604713379\n",
      "235 0.5119399380510412\n",
      "236 0.4884379703652334\n",
      "237 0.46603054433355967\n",
      "238 0.444664809902497\n",
      "239 0.42429280910130707\n",
      "240 0.40486502107689276\n",
      "241 0.38632967289632586\n",
      "242 0.36865417600617734\n",
      "243 0.3517949146422401\n",
      "244 0.33571834848187876\n",
      "245 0.32039165371586487\n",
      "246 0.30576506261261593\n",
      "247 0.29181197983957685\n",
      "248 0.2785018417953059\n",
      "249 0.2658065841711882\n",
      "250 0.2536952920377487\n",
      "251 0.24214986791773976\n",
      "252 0.23112659653785494\n",
      "253 0.22061011899691563\n",
      "254 0.21057685130947645\n",
      "255 0.20100497182683802\n",
      "256 0.19187359652707786\n",
      "257 0.18316459682908487\n",
      "258 0.17485041497042642\n",
      "259 0.16691590456724284\n",
      "260 0.15934518869523534\n",
      "261 0.15212143724443908\n",
      "262 0.14522883475010961\n",
      "263 0.13865364644935227\n",
      "264 0.132376353797811\n",
      "265 0.12638463431081573\n",
      "266 0.12066697204870754\n",
      "267 0.115211365260579\n",
      "268 0.11000322623697567\n",
      "269 0.10503416376919093\n",
      "270 0.10029084355712689\n",
      "271 0.09576251312068101\n",
      "272 0.091441572456464\n",
      "273 0.08731627728993911\n",
      "274 0.08337835891833384\n",
      "275 0.07962019531568675\n",
      "276 0.07603318515404908\n",
      "277 0.07260840010753192\n",
      "278 0.06933918188447122\n",
      "279 0.06621759012401288\n",
      "280 0.06323803167597743\n",
      "281 0.060393808706474594\n",
      "282 0.05767941427928887\n",
      "283 0.05508747022847739\n",
      "284 0.052612054088445444\n",
      "285 0.05024841642071767\n",
      "286 0.04799188258386154\n",
      "287 0.04583738799976634\n",
      "288 0.04378191201731396\n",
      "289 0.041817824740808404\n",
      "290 0.03994239858356609\n",
      "291 0.03815153172674342\n",
      "292 0.03644152920977309\n",
      "293 0.0348092508234007\n",
      "294 0.03325080076744786\n",
      "295 0.031762054707694894\n",
      "296 0.030340300838472955\n",
      "297 0.028982506763596393\n",
      "298 0.027686123876959852\n",
      "299 0.02644804311448961\n",
      "300 0.02526586967057005\n",
      "301 0.024136860097111648\n",
      "302 0.023058303844521354\n",
      "303 0.022028394611120536\n",
      "304 0.021044810172046652\n",
      "305 0.020105214782460804\n",
      "306 0.019207988016512276\n",
      "307 0.01835129927380092\n",
      "308 0.017532700577737474\n",
      "309 0.01675110066963225\n",
      "310 0.016004248856427008\n",
      "311 0.015290882462798099\n",
      "312 0.01460959621065729\n",
      "313 0.013959156211428288\n",
      "314 0.013337655241797496\n",
      "315 0.012743818591979266\n",
      "316 0.012176578463656506\n",
      "317 0.011634681024517379\n",
      "318 0.01111702664067774\n",
      "319 0.010622865158684582\n",
      "320 0.010150612742747662\n",
      "321 0.009699336309578213\n",
      "322 0.009268236445441575\n",
      "323 0.008856414429735521\n",
      "324 0.008463050678683507\n",
      "325 0.008087282235613811\n",
      "326 0.0077283502587073396\n",
      "327 0.007385280720218006\n",
      "328 0.007057507212679289\n",
      "329 0.006744393010780813\n",
      "330 0.00644527582086186\n",
      "331 0.0061594205234970605\n",
      "332 0.0058864582529366645\n",
      "333 0.0056255373935575476\n",
      "334 0.005376244366775921\n",
      "335 0.005138107664316221\n",
      "336 0.00491048788936495\n",
      "337 0.004693011517314933\n",
      "338 0.004485276706628721\n",
      "339 0.004286772074149494\n",
      "340 0.00409709596271833\n",
      "341 0.003915810972592003\n",
      "342 0.0037425708786062778\n",
      "343 0.0035770313871379524\n",
      "344 0.0034188706711228084\n",
      "345 0.003267853360298542\n",
      "346 0.0031234162073793736\n",
      "347 0.0029853788743781527\n",
      "348 0.0028534757140765473\n",
      "349 0.0027274263929039587\n",
      "350 0.002606989346398207\n",
      "351 0.0024919343028845685\n",
      "352 0.0023819547916651565\n",
      "353 0.0022768190741883445\n",
      "354 0.002176343318642684\n",
      "355 0.0020803270519554485\n",
      "356 0.0019885897944423446\n",
      "357 0.0019008894348884874\n",
      "358 0.001817120642112734\n",
      "359 0.0017370118886509563\n",
      "360 0.001660464281982459\n",
      "361 0.0015873115958551266\n",
      "362 0.0015173778446729898\n",
      "363 0.001450538331676228\n",
      "364 0.0013866733560749902\n",
      "365 0.0013256334456274337\n",
      "366 0.0012672835915465063\n",
      "367 0.0012115093569738632\n",
      "368 0.001158195999025147\n",
      "369 0.001107234380956043\n",
      "370 0.001058524308825288\n",
      "371 0.0010119932354764908\n",
      "372 0.000967507841297222\n",
      "373 0.000924968690221892\n",
      "374 0.0008843073722222282\n",
      "375 0.0008454417924409954\n",
      "376 0.0008082954797703171\n",
      "377 0.000772801973181319\n",
      "378 0.0007388720468943663\n",
      "379 0.0007064206127307644\n",
      "380 0.0006754022545816521\n",
      "381 0.000645749781976472\n",
      "382 0.0006174093126481448\n",
      "383 0.0005903166394800529\n",
      "384 0.0005644229946502601\n",
      "385 0.0005396709767853832\n",
      "386 0.0005159980913515526\n",
      "387 0.0004933693745356887\n",
      "388 0.00047174198899220526\n",
      "389 0.0004510611336424799\n",
      "390 0.00043128888167884543\n",
      "391 0.0004123957948574307\n",
      "392 0.0003943277161030137\n",
      "393 0.0003770563110317936\n",
      "394 0.0003605412276430259\n",
      "395 0.0003447505173435787\n",
      "396 0.0003296539236105748\n",
      "397 0.00031522179510973426\n",
      "398 0.0003014324316855828\n",
      "399 0.00028824288206154617\n",
      "400 0.000275629543994023\n",
      "401 0.000263570322960904\n",
      "402 0.00025204074312653685\n",
      "403 0.0002410167196322194\n",
      "404 0.0002304801546545154\n",
      "405 0.000220408983275332\n",
      "406 0.00021077370669107736\n",
      "407 0.00020156087281316955\n",
      "408 0.00019275349347932423\n",
      "409 0.00018433202566564788\n",
      "410 0.00017628223965047144\n",
      "411 0.00016858258750018407\n",
      "412 0.0001612234432034035\n",
      "413 0.00015418307111087607\n",
      "414 0.0001474515313352822\n",
      "415 0.00014101674581264563\n",
      "416 0.00013486245003449258\n",
      "417 0.00012897695288271012\n",
      "418 0.00012334983593448133\n",
      "419 0.00011797141776310793\n",
      "420 0.00011282578342460043\n",
      "421 0.00010790664170128749\n",
      "422 0.00010320117845184781\n",
      "423 9.870185293903012e-05\n",
      "424 9.439926767807198e-05\n",
      "425 9.028515607637826e-05\n",
      "426 8.635325754963874e-05\n",
      "427 8.259129199978891e-05\n",
      "428 7.899338099809822e-05\n",
      "429 7.555250685617714e-05\n",
      "430 7.226193174400776e-05\n",
      "431 6.911526236357804e-05\n",
      "432 6.610708055928251e-05\n",
      "433 6.32310839546013e-05\n",
      "434 6.047905755892122e-05\n",
      "435 5.78471398833916e-05\n",
      "436 5.533002841950454e-05\n",
      "437 5.292304144820684e-05\n",
      "438 5.0621807771851675e-05\n",
      "439 4.842019848946907e-05\n",
      "440 4.6315586533164065e-05\n",
      "441 4.430191495220948e-05\n",
      "442 4.237596713831772e-05\n",
      "443 4.053433304263541e-05\n",
      "444 3.877289265903281e-05\n",
      "445 3.7087974856000536e-05\n",
      "446 3.5476554563724224e-05\n",
      "447 3.393607927444057e-05\n",
      "448 3.2462210454014454e-05\n",
      "449 3.105274856123027e-05\n",
      "450 2.970440954279785e-05\n",
      "451 2.8414713384589116e-05\n",
      "452 2.7181196769077024e-05\n",
      "453 2.6001373738806413e-05\n",
      "454 2.4873364444672365e-05\n",
      "455 2.379469647926504e-05\n",
      "456 2.2762328569554604e-05\n",
      "457 2.1774879714246218e-05\n",
      "458 2.08304221088189e-05\n",
      "459 1.992711243567925e-05\n",
      "460 1.9063100680797677e-05\n",
      "461 1.8237000836881564e-05\n",
      "462 1.7446764562230823e-05\n",
      "463 1.6690538127800117e-05\n",
      "464 1.5967178457539786e-05\n",
      "465 1.527529025727676e-05\n",
      "466 1.4613584546209841e-05\n",
      "467 1.398068405677266e-05\n",
      "468 1.3375223926436748e-05\n",
      "469 1.2796259148602341e-05\n",
      "470 1.2242193071971e-05\n",
      "471 1.1712162764424738e-05\n",
      "472 1.1205218692452546e-05\n",
      "473 1.072030698233537e-05\n",
      "474 1.0256338303379872e-05\n",
      "475 9.812518045334534e-06\n",
      "476 9.388108277798727e-06\n",
      "477 8.982079247366746e-06\n",
      "478 8.593642571571787e-06\n",
      "479 8.22203144704041e-06\n",
      "480 7.866485141168102e-06\n",
      "481 7.526373459906368e-06\n",
      "482 7.2009877461529635e-06\n",
      "483 6.88974784753089e-06\n",
      "484 6.592230341657647e-06\n",
      "485 6.307386456339183e-06\n",
      "486 6.034862875437371e-06\n",
      "487 5.774134981552618e-06\n",
      "488 5.524700519385731e-06\n",
      "489 5.286080051992187e-06\n",
      "490 5.057852539580296e-06\n",
      "491 4.839537869930848e-06\n",
      "492 4.63067756410502e-06\n",
      "493 4.430772717010027e-06\n",
      "494 4.239521557822185e-06\n",
      "495 4.056554325085784e-06\n",
      "496 3.881570859255379e-06\n",
      "497 3.714129477967302e-06\n",
      "498 3.55390303715725e-06\n",
      "499 3.4006607679013205e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    \n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0) # relu\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # 反向传播\n",
    "    grad_y_pred = 2.0*(y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "## Pytorch:Tensors 和 autograd\n",
    "\n",
    "上面示例中使用numpy手动实现了forward 和 backward, 当网络复杂时，手动实现反向传播将非常复杂。\n",
    "\n",
    "pytorch提供了自动微分计算实现反向传播。通过构建计算图，节点为张量，连接边为函数。\n",
    "\n",
    "如果`x`是个张量，只需要将`x.requires_grad=True`,然后`x.grad`就可以用来记录梯度。\n",
    "\n",
    "用pytorch实现的网络如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0)\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建输入输出，默认的require_grad为False\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机创建权重\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 34.13650131225586\n",
      "199 1.8900409936904907\n",
      "299 0.1125074028968811\n",
      "399 0.007219775579869747\n",
      "499 0.0007748950738459826\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # 计算并打印loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # 使用autograd计算反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 手动更新loss，因为权重的requires_grad为ture，在更新操作不需要更新，\n",
    "    # 所以使用torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 更新权重后，手动把梯度清零\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch:定义新的autograd函数\n",
    "\n",
    "每一个autograd函数都提供两个函数操作张量，forward函数根据输入计算输出张量，backward反向计算梯度。\n",
    "\n",
    "在pytorch中通过定义`torch.autograd.Function`的子类，实现自己的autograd操作，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRelu(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建输入输出，默认的require_grad为False\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机创建权重\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 619.879638671875\n",
      "199 4.068207263946533\n",
      "299 0.03632698208093643\n",
      "399 0.0006286892457865179\n",
      "499 7.912427099654451e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    relu = MyRelu.apply\n",
    "    \n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 更新权重后，手动把梯度清零\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn 模块\n",
    "## Pytorch：nn\n",
    "\n",
    "`nn`包定义了一系列的模块，它大致相当于网络层。模块接收输入张量并计算输出张量，但也可以保持内部状态，例如包含可学习参数的张量。nn包还定义了一组在训练神经网络时常用的有用损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 与上面相同的部分\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包定义模型，模型为层的序列，每个线性层包含参数\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.387185573577881\n",
      "199 0.03280126675963402\n",
      "299 0.0011327475076541305\n",
      "399 5.52507808606606e-05\n",
      "499 3.063823896809481e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # 在反向传播前将参数的梯度归0，对应上面最后把w1, w2置0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch:optim\n",
    "\n",
    "到目前为止，都是手动更新参数。对于像随机梯度下降这样的简单优化算法来说，这不是一个巨大的负担，但在实践中，我们经常使用更复杂的优化器如AdaGrad，RMSProp，Adam等来训练神经网络。\n",
    "\n",
    "`optim`提供了常见优化算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 与上面相同的部分\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包定义模型，模型为层的序列，每个线性层包含参数\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 50.85662078857422\n",
      "199 0.897648811340332\n",
      "299 0.005331294145435095\n",
      "399 2.724405931076035e-05\n",
      "499 9.765761888047564e-08\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # 在反向传播前将参数的梯度归0，对应上面最后把w1, w2置0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # 使用step函数，逐步更新梯度\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch：自定义 nn 模块\n",
    "\n",
    "有时需要指定比现有模块序列更复杂的模型;通过继承 `nn`并实现`forward`实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.0103516578674316\n",
      "199 0.028675688430666924\n",
      "299 0.0007557850331068039\n",
      "399 2.5025467039085925e-05\n",
      "499 9.620515584174427e-07\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch:控制流 + 权重共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # 每次前向生成一个随机数，即n个隐藏层，每次的层数不一样，\n",
    "        # 但是每次对应层的参数是相同的\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        \n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = DynamicNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 68.56875610351562\n",
      "199 25.789127349853516\n",
      "299 1.280673623085022\n",
      "399 1.2233548164367676\n",
      "499 0.24811068177223206\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
