# 神经网络
通过`torch.nn`包可以构建神经网络。

![network](../images/mnist.png)
如图所示是一个用于手写字符识别的简单卷积分类网络，这是简单的前馈网络，输入经过几个网络层后得到输出。 
一个神经网络的训练流程通常是：
- 定义包含学习参数的神经网络
- 迭代输入数据集
- 通过网络处理输入
- 计算loss
- 反向传播
- 更新参数

## 定义网络
```python
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        # 定义一个卷积层结构
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)

        # 全连接
        self.fc1 = nn.Linear(16*6*6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 卷积后接激活接最大池化
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))

        # 将2d拉伸为1d
        x = x.view(-1, self.num_flat_features(x))

        # 全连接接激活
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


if __name__ == "__main__":
    net = Net()
    print(net)
```
```
out:
Net(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
```
> 在这里我们只需要定义`forward`函数, `backward`在我们使用`autograd`时会自动生成。
> 整个模型的可训练参数，通过`net.parameters()`返回
```python
params = list(net.parameters())
print(len(params))
print(params[0].size())
```
```
out:
    10
    torch.Size([6, 1, 3, 3])
```
> 通过随机产生一个张量作为输入，根据网络得到张量大小必须是32*32
```python
input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
```
```
tensor([[ 0.0306, -0.0599, -0.0360,  0.0910, -0.1089,  0.0825,  0.0066,  0.0956,
          0.0480,  0.0636]], grad_fn=<AddmmBackward>)
```
> 清空所有参数的梯度缓存,使用随机梯度进行反向传播
```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```
> `torch.nn`仅支持mini-batch，不支持单个样本的输入，如果只有一个样本，需要使用`input.unsqueeze(0)`添加假的batch维度

## loss 

loss函数使用(output, target)作为输入，评估两者之间的距离作为输出。nn中包含许多loss函数
```python
input = torch.randn(1, 1, 32, 32)
out = net(input)

# 随机生成标签
target = torch.randn(10)
target = target.view(1, -1)

# 定义loss函数
criterion = nn.MSELoss()
loss = criterion(out, target)
print(loss)
```
```
tensor(0.9280, grad_fn=<MseLossBackward>)
```
> 当我们调用loss.barkward(),整个计算图将进行反向传播，每个requires_grad属性为true的张量，都将得到他们的梯度。
```python
print(loss.grad_fn)
print(loss.grad_fn.next_functions[0][0])
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])
```
```
<MseLossBackward object at 0x000001E5CAB151D0>
<AddmmBackward object at 0x000001E5CAB15E10>
<AccumulateGrad object at 0x000001E5CAB15E10>
```

## 反向传播
在进行反向之前，需要清理现有的梯度，否则梯度将累计在现有之上。
```python
net.zero_grad()
print(net.conv1.bias.grad)
loss.backward()
print(net.conv1.bias.grad)
```
```
None
tensor([ 0.0015, -0.0111,  0.0086, -0.0064,  0.0066,  0.0091])
```

## 更新权重
一种简单的更新方式是SGD
```math
weight = weight - learning_rate * gradient
```
```python
learning_rate = 0.01
if f in net.parameters():
    f.data.sub_(f.grad * learning_rate)
```
在pytorh中已经集成了许多参数更新方法，需要用到optim包
```python
import torch.optim import optim

optimizer = optim.SGD(net.parameters(), lr=0.01)
optim.zero_grad()
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()
```